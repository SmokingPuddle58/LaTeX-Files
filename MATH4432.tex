%!TEX program = lualatex
\documentclass{article}
\usepackage{tcolorbox}
\usepackage{ntheorem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ulem}
\usepackage{graphicx}
\usepackage{centernot}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{tabularx}
\usepackage{pgfcore}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage[hidelinks, linktocpage]{hyperref}
\hypersetup{
    linktoc=all,
}

\usepackage[margin=1in]{geometry}
\usepackage[parfill]{parskip}

\everymath{\displaystyle}

\makeatletter
\newtheoremstyle{MyNonumberplain}%
  {\item[\theorem@headerfont\hskip\labelsep ##1\theorem@separator]}%
  {\item[\theorem@headerfont\hskip\labelsep ##3\theorem@separator]}%
\makeatother
\theoremstyle{MyNonumberplain}
\theorembodyfont{\upshape}

\theoremstyle{break}
\newtheorem*{proof}{Proof. }

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\nin}{\not\in}
\newcommand{\p}{\phi}
\newcommand{\ev}{\mathbb{E}}
\newcommand{\var}{\text{Var}}




\newtcolorbox{prfbox}{colback=gray!10,colframe=black!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=0pt}
\newtcolorbox{thmbox}{colback=orange!25,colframe=orange!85,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}
\newtcolorbox{defbox}{colback=blue!5,colframe=blue!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}
\newtcolorbox{ansbox}{colback=gray!10,colframe=black!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=0pt}
\newtcolorbox{expbox}{colback=green!10,colframe=green!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}
\newtcolorbox{warnbox}{colback=red!15,colframe=red!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}


\newtheorem{warning}{Warning}[section]
\newtheorem{remark}{Remark}[section]
\theoremstyle{break}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{break}
%\theoremstyle{definition}
\theoremstyle{break}
\newtheorem{definition}{Definition}[section]

\title{MATH4432 Notes}
\author{SmokingPuddle58}

\begin{document}

\maketitle

\begin{center}
    This work is licensed under CC BY-NC-SA 4.0
\end{center}


\newpage

    This is the lecture note typed by SmokingPuddle in September, 2024. It mainly contains what professor mentions starting from year 3. For the contents of the first two weeks, I will try my best to include as much as possible. 

    The main reference source comes from the professor himself, lecture notes, tutorial notes, and also from the Internet if necessary.

    Please inform me if there is any errors, better within the semester or I will have a very high chance of forgetting the contents. 

    \bigskip


\begin{thmbox}
    Theorems, Corollary, Lemma, Proposition
\end{thmbox}

\begin{defbox}
    Definitions
\end{defbox}

\begin{expbox}
    Examples
\end{expbox}

\begin{warnbox}
    Warnings / Remarks
\end{warnbox}

\begin{prfbox}
    Proofs, Answers
\end{prfbox}

\begin{tabular}{ll}
    &\\
\end{tabular}

Some special symbols, notations and functions that will appear in this note:\bigskip

\begin{center}

    \begin{tabular}{|l|l|}
        \hline
        $\C$ & Set of complex numbers \\ \hline
        $\R$ & Set of real numbers \\ \hline
        $\Z$ & Set of integers \\ \hline
        $\Q$ & Set of rational numbers \\ \hline
    \end{tabular}
\end{center}
\begin{center}
    

\end{center}


\newpage

\tableofcontents

\newpage

% If you wish your section number begins from 1, remove / comment the line below
% \setcounter{section}{-1}

\section{Overview}

\subsection{Introduction}

Before we start, we shall clarify some of the notations that will be used.

Consider the following expression: $$P(X=x)$$

If we say $r.v.$ (Random variable) $X$, we actually means the name of the variable, while for $x$, we means the realization for such $r.v.$ 

Suppose we are now observing some quantitative response $Y$ and also input variable $X$, consisting of $p$ features, which can be expressed as:

$$
X=
    \begin{bmatrix}
        X_1 \\
        X_2 \\
        X_3 \\
        \vdots \\
        X_p
    \end{bmatrix}
$$

where $X_1,...,X_p$ are random variables. Then the relation between $Y$ and $X$ can be expressed as: 

$$
    Y=f(X)+\varepsilon
$$

where $\varepsilon$ is the error term independent from $X$, with mean 0, and $f$ is a deterministic function. We call such model the population level model, or ground truth model. (i.e. The number of samples is infinitely many)

\begin{warnbox}
    \begin{remark}
        Note that $Y,\varepsilon$ are all random variables, while $X$ is a collection of random variables.
    \end{remark}
\end{warnbox}

If we want to consider a sample level (the realization of the random variables), then the equation becomes:
    \begin{eqnarray*}
             y_i= & f(x_i)+\varepsilon_i & i=1,...,n
    \end{eqnarray*}

where $x_i$ can be a vector like the following:

$$
    x_i=
    \begin{bmatrix}
        {x_i}_1 \\
        {x_i}_2 \\
        {x_i}_3 \\
        \vdots \\
        {x_i}_p
    \end{bmatrix}
$$

and $n$ is the sample size.

\begin{warnbox}
    \begin{remark}
        In machine learning, vectors usually means \textbf{column vectors, but not row vectors}.
    \end{remark}
    For example, consider the equation $f(x)=a_1 x_1+a_2 x_2 + a_3 x_3$.
    If we know that 
    $
    a=    
    \begin{bmatrix}
        a_1 \\
        a_2 \\
        a_3
    \end{bmatrix}
    $,
        $
    x=    
    \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3
    \end{bmatrix}
    $,
    then $f(x)=a^\intercal x$, where $a^\intercal$ is the transpose of the vector $a$.
\end{warnbox}

Now let's go back to the ground truth model, which is $Y=f(X)+\varepsilon$. Suppose we want to construct $f$ from the data, then we will have: 
$$
    \hat{Y}=\hat{f}(X=x)
$$

for any observed $x$.

Suppose we are interested in the difference between the data and the observed prediction, then we will be interested in the value of $\ev(Y-\hat{Y})^2$, the expected square error.

\begin{warnbox}
    \begin{remark}
        Both $Y$ and $\hat{Y}$ are random variable, since $\hat{Y}$ is the prediction that is learnt from the data, and data comes from the random sample chosen from ground truth model. 
        Thus we are not interested in the value of $(Y-\hat{Y})^2$, since it is not fixed.
    \end{remark}
\end{warnbox}

\begin{thmbox}
    \begin{theorem}
    \begin{eqnarray*}
        \ev(Y-\hat{Y})^2 & = & \underbrace{\ev(f(X)-\hat{f}(X))^2}_{\text{Reducible}} \text{ } + \underbrace{\text{ Var}(\varepsilon)}_{\text{Irreducible}}
    \end{eqnarray*}
    \end{theorem}
    \begin{prfbox}
        \begin{proof}
            \begin{eqnarray*}
                \ev(Y-\hat{Y})^2 & = & \ev(f(X)+\varepsilon-\hat{f}(X))^2 \\
                                 & = & \ev(f(X)-\hat{f}(X)+\varepsilon)^2 \\
                                 & = & \ev((f(X)-\hat{f}(X))^2+2\varepsilon(f(X)-\hat{f}(X))+\varepsilon^2) \\
                                 & = & \ev((f(X)-\hat{f}(X))^2)+\ev(2\varepsilon(f(X)-\hat{f}(X)))+\ev(\varepsilon^2) \\
                                 & = & \ev((f(X)-\hat{f}(X))^2)+\underbrace{\ev(2\varepsilon)\ev((f(X)-\hat{f}(X)))}_{\substack{\text{Assume }\varepsilon \text{ independent from } \\ f,\hat{f}}}+\ev(\varepsilon^2) \\
                                 & = & \ev(f(X)-\hat{f}(X))^2+\underbrace{0}_{\substack{\ev(\varepsilon)=0}}+\ev(\varepsilon^2) 
            \end{eqnarray*}
            Since for any random variable $X$, and its expected value, $E(X)=\mu$, we have: (Covered in MATH2411)
            \begin{eqnarray*}
                \text{Var}(X) & = & \ev(X-\mu)^2\\
                              & = & \ev(X^2)+\ev(\mu^2)-2\ev(X\mu)\\
                              & = & \ev(X^2)+\mu^2-2\mu\ev(X)\\
                              & = & \ev(X^2)+\mu^2-2\mu^2\\
                              & = & \ev(X^2)-\mu^2
            \end{eqnarray*}
            Thus, 
            \begin{eqnarray*}
                \ev(Y-\hat{Y})^2 & = & \ev(f(X)-\hat{f}(X))^2+\ev(\varepsilon^2) \\
                                 & = & \ev(f(X)-\hat{f}(X))^2+\var(\varepsilon)
            \end{eqnarray*}
        \end{proof}
    \end{prfbox}
\end{thmbox}

To conclude, you can only reduce the error for the reducible part, by making your model approximate the ground truth model as well as possible, 
while we can really do not much on the irreducible part.

\subsection{Estimation of $f$}

There are two methods for estimating $f$, namely parametric, and non-parametric methods.

For parametric method, we assume that $f$ can be described by a set of parameters, such that once all of the parameters are known, then the model is known.

\begin{expbox}
    \begin{example}
        One of the most simple assumption is that $f$ is linear in $X$, which can be described as:  $$f(X)=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p$$
    \end{example}
\end{expbox}

For non-parametric method, we do not pre-specify the form of the model (Can be linear, non-linear. tree and neural network). To 
control the flexibility, we can always tune the parameters.

The advantages and disadvantages are listed in the following table.

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
                    & Parametric                                       & Non-parametric                                          \\ \hline
    Advantage    & Easy to solve and understand                     & More flexible and sometimes more powerful               \\ \hline
    Disadvantage & \makecell{The model may be too simple\\ to fit into the data} & \makecell{The model may be too flexible, \\there may be overfitting} \\ \hline
    \end{tabular}
\end{table}

\begin{center}
    \includegraphics*[scale=0.35]{img1.png}
\end{center}

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    Ground truth model & Underfitting                         \\ \hline
    Good estimation    & Overfitting (Fitting into the noise) \\ \hline
    \end{tabular}
\end{table}

The above image shows the example of a ground truth model, a good estimation, overfitting and underfitting. It is also included in the lecture 
note. 

\end{document}

% Below are some boxes that may help with your document. They are not rendered during compilation.

% Definition
\begin{defbox}
    \begin{definition}[]

    \end{definition}
\end{defbox}

% Theorem (With a proof box included, remove if necessary)
\begin{thmbox}
    \begin{theorem}

    \end{theorem}
    \begin{prfbox}
        \begin{proof}

        \end{proof}
    \end{prfbox}
\end{thmbox}

% Example
\begin{expbox}
    \begin{example}

    \end{example}
\end{expbox}

% Warning / Remarks
\begin{warnbox}
    \begin{warning}

    \end{warning}
\end{warnbox}

\begin{warnbox}
    \begin{remark}

    \end{remark}
\end{warnbox}