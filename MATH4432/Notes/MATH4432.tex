% !TeX program = lualatex
\documentclass{article}
\usepackage{tcolorbox}
\usepackage{ntheorem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ulem}
\usepackage{graphicx}
\usepackage{centernot}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage{emoji}
\usepackage[hidelinks, linktocpage]{hyperref}
\hypersetup{
    linktoc=all,
}

\usepackage[margin=1in]{geometry}
\usepackage[parfill]{parskip}

\everymath{\displaystyle}

\makeatletter
\newtheoremstyle{MyNonumberplain}%
  {\item[\theorem@headerfont\hskip\labelsep ##1\theorem@separator]}%
  {\item[\theorem@headerfont\hskip\labelsep ##3\theorem@separator]}%
\makeatother
\theoremstyle{MyNonumberplain}
\theorembodyfont{\upshape}

\theoremstyle{break}
\newtheorem*{proof}{Proof. }

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\nin}{\not\in}
\newcommand{\p}{\phi}
\newcommand{\ev}{\mathbb{E}}
\newcommand{\var}{\text{Var}}
\newcommand{\der}{\operatorname{d\!}{}}
\newcommand{\evd}{\ev_{\mathcal{D}}}
\newcommand{\bias}{\text{Bias}}
\newcommand{\bt}[1]{\beta_{#1}}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}




\newtcolorbox{prfbox}{colback=gray!10,colframe=black!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=0pt}
\newtcolorbox{thmbox}{colback=orange!25,colframe=orange!85,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}
\newtcolorbox{defbox}{colback=blue!5,colframe=blue!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}
\newtcolorbox{ansbox}{colback=gray!10,colframe=black!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=0pt}
\newtcolorbox{expbox}{colback=green!10,colframe=green!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}
\newtcolorbox{warnbox}{colback=red!15,colframe=red!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}
\newtcolorbox{notebox}{colback=magenta!15,colframe=magenta!70,boxrule=0pt,arc=0pt,boxsep=2pt,left=2pt,right=2pt,leftrule=2.5pt}

\newtheorem{warning}{Warning}[section]
\newtheorem{remark}{Remark}[section]
\theoremstyle{break}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{note}{Note}
\newtheorem*{question}{Question}
\newtheorem*{answer}{Answer}



\theoremstyle{break}
%\theoremstyle{definition}
\theoremstyle{break}
\newtheorem{definition}{Definition}[section]

\title{MATH4432 Notes}
\author{SmokingPuddle58}

\begin{document}

\maketitle

\begin{center}
    This work is licensed under CC BY-NC-SA 4.0
\end{center}


\newpage

    This is the lecture note typed by SmokingPuddle in September, 2024. It mainly contains what professor mentions starting from year 3. For the contents of the first two weeks, I will try my best to include as much as possible. 

    The main reference source comes from the professor himself, lecture notes, tutorial notes, and also from the Internet if necessary.

    Please inform me if there is any errors, better within the semester or I will have a very high chance of forgetting the contents. 

    \bigskip


\begin{thmbox}
    Theorems, Corollary, Lemma, Proposition
\end{thmbox}

\begin{defbox}
    Definitions
\end{defbox}

\begin{expbox}
    Examples
\end{expbox}

\begin{warnbox}
    Warnings / Remarks
\end{warnbox}

\begin{prfbox}
    Proofs, Answers
\end{prfbox}

\begin{tabular}{ll}
    &\\
\end{tabular}

Some special symbols, notations and functions that will appear in this note:\bigskip

\begin{center}

    \begin{tabular}{|l|l|}
        \hline
        $\C$ & Set of complex numbers \\ \hline
        $\R$ & Set of real numbers \\ \hline
        $\Z$ & Set of integers \\ \hline
        $\Q$ & Set of rational numbers \\ \hline
    \end{tabular}
\end{center}
\begin{center}
    

\end{center}

\newpage

\tableofcontents

\newpage

% If you wish your section number begins from 1, remove / comment the line below
% \setcounter{section}{-1}

\section{Overview}

\subsection{Introduction}

Before we start, we shall clarify some of the notations that will be used.

Consider the following expression: $$P(X=x)$$

If we say $r.v.$ (Random variable) $X$, we actually means the name of the variable, while for $x$, we means the realization for such $r.v.$ 

Suppose we are now observing some quantitative response $Y$ and also input variable $X$, consisting of $p$ features, which can be expressed as:

$$
X=
    \begin{bmatrix}
        X_1 \\
        X_2 \\
        X_3 \\
        \vdots \\
        X_p
    \end{bmatrix}
$$

where $X_1,...,X_p$ are random variables. Then the relation between $Y$ and $X$ can be expressed as: 

$$
    Y=f(X)+\varepsilon
$$

where $\varepsilon$ is the error term independent from $X$, with mean 0, and $f$ is a deterministic function. We call such model the population level model, or ground truth model. (i.e. The number of samples is infinitely many)

\begin{warnbox}
    \begin{remark}
        Note that $Y,\varepsilon$ are all random variables, while $X$ is a collection of random variables.
    \end{remark}
\end{warnbox}

If we want to consider a sample level (the realization of the random variables), then the equation becomes:
    \begin{eqnarray*}
             y_i= & f(x_i)+\varepsilon_i & i=1,...,n
    \end{eqnarray*}

where $x_i$ can be a vector like the following:

$$
    x_i=
    \begin{bmatrix}
        {x_i}_1 \\
        {x_i}_2 \\
        {x_i}_3 \\
        \vdots \\
        {x_i}_p
    \end{bmatrix}
$$

and $n$ is the sample size.

\begin{warnbox}
    \begin{remark}
        In machine learning, vectors usually means \textbf{column vectors, but not row vectors}.
    \end{remark}
    For example, consider the equation $f(x)=a_1 x_1+a_2 x_2 + a_3 x_3$.
    If we know that 
    $
    a=    
    \begin{bmatrix}
        a_1 \\
        a_2 \\
        a_3
    \end{bmatrix}
    $,
        $
    x=    
    \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3
    \end{bmatrix}
    $,
    then $f(x)=a^\intercal x$, where $a^\intercal$ is the transpose of the vector $a$.
\end{warnbox}

Now let's go back to the ground truth model, which is $Y=f(X)+\varepsilon$. Suppose we want to construct $f$ from the data, then we will have: 
$$
    \hat{Y}=\hat{f}(X=x)
$$

for any observed $x$.

Suppose we are interested in the difference between the data and the observed prediction, then we will be interested in the value of $\ev(Y-\hat{Y})^2$, the expected square error.

\begin{warnbox}
    \begin{remark}
        Both $Y$ and $\hat{Y}$ are random variable, since $\hat{Y}$ is the prediction that is learnt from the data, and data comes from the random sample chosen from ground truth model. 
        Thus we are not interested in the value of $(Y-\hat{Y})^2$, since it is not fixed.
    \end{remark}
\end{warnbox}

\begin{thmbox}
    \begin{theorem}
    \begin{eqnarray*}
        \ev(Y-\hat{Y})^2 & = & \underbrace{\ev(f(X)-\hat{f}(X))^2}_{\text{Reducible}} \text{ } + \underbrace{\text{ Var}(\varepsilon)}_{\text{Irreducible}}
    \end{eqnarray*}
    \end{theorem}
    \begin{prfbox}
        \begin{proof}
            \begin{eqnarray*}
                \ev(Y-\hat{Y})^2 & = & \ev(f(X)+\varepsilon-\hat{f}(X))^2 \\
                                 & = & \ev(f(X)-\hat{f}(X)+\varepsilon)^2 \\
                                 & = & \ev((f(X)-\hat{f}(X))^2+2\varepsilon(f(X)-\hat{f}(X))+\varepsilon^2) \\
                                 & = & \ev((f(X)-\hat{f}(X))^2)+\ev(2\varepsilon(f(X)-\hat{f}(X)))+\ev(\varepsilon^2) \\
                                 & = & \ev((f(X)-\hat{f}(X))^2)+\underbrace{\ev(2\varepsilon)\ev((f(X)-\hat{f}(X)))}_{\substack{\text{Assume }\varepsilon \text{ independent from } \\ f,\hat{f}}}+\ev(\varepsilon^2) \\
                                 & = & \ev(f(X)-\hat{f}(X))^2+\underbrace{0}_{\substack{\ev(\varepsilon)=0}}+\ev(\varepsilon^2) 
            \end{eqnarray*}
            Since for any random variable $X$, and its expected value, $E(X)=\mu$, we have: (Covered in MATH2411)
            \begin{eqnarray*}
                \text{Var}(X) & = & \ev(X-\mu)^2\\
                              & = & \ev(X^2)+\ev(\mu^2)-2\ev(X\mu)\\
                              & = & \ev(X^2)+\mu^2-2\mu\ev(X)\\
                              & = & \ev(X^2)+\mu^2-2\mu^2\\
                              & = & \ev(X^2)-\mu^2
            \end{eqnarray*}
            Thus, 
            \begin{eqnarray*}
                \ev(Y-\hat{Y})^2 & = & \ev(f(X)-\hat{f}(X))^2+\ev(\varepsilon^2) \\
                                 & = & \ev(f(X)-\hat{f}(X))^2+\var(\varepsilon)
            \end{eqnarray*}
        \end{proof}
    \end{prfbox}
\end{thmbox}

To conclude, you can only reduce the error for the reducible part, by making your model approximate the ground truth model as well as possible, 
while we can really do not much on the irreducible part.

\subsection{Estimation of $f$}

There are two methods for estimating $f$, namely parametric, and non-parametric methods.

For parametric method, we assume that $f$ can be described by a set of parameters, such that once all of the parameters are known, then the model is known.

\begin{expbox}
    \begin{example}
        One of the most simple assumption is that $f$ is linear in $X$, which can be described as:  $$f(X)=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p$$
    \end{example}
\end{expbox}

For non-parametric method, we do not pre-specify the form of the model (Can be linear, non-linear. tree and neural network). To 
control the flexibility, we can always tune the parameters.

The advantages and disadvantages are listed in the following table.

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
                    & Parametric                                       & Non-parametric                                          \\ \hline
    Advantage    & Easy to solve and understand                     & More flexible and sometimes more powerful               \\ \hline
    Disadvantage & \makecell{The model may be too simple\\ to fit into the data} & \makecell{The model may be too flexible, \\there may be overfitting} \\ \hline
    \end{tabular}
\end{table}

\begin{center}
    \includegraphics*[scale=0.35]{Images/img1.png}
\end{center}

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    Ground truth model & Underfitting                         \\ \hline
    Good estimation    & Overfitting (Fitting into the noise) \\ \hline
    \end{tabular}
\end{table}

The above image shows the example of a ground truth model, a good estimation, overfitting and underfitting. It is also included in the lecture 
note. 

\subsection{Assessing Model Accuracy}

In statistical machine learning context, to find a good estimate $f$, we shall introduce the concept of regularization 
(Covered in detail later), in which we want to minimize the following value as much as possible:
$$\text{Loss}(Y,f)+\lambda R(f)$$
where $Y$ is the response, $\lambda$ is a tuning parameter (weight) for the regularizer $R(f)$ of the model $f$.

The introduction of the regularizer is trying to control the complexity / flexibility of the function $f$ to prevent perfect fit / overfitting issue.

\begin{expbox}
    \begin{example}[Spline interpolation]
        Consider the loss function and the regularizer as: $$\underbrace{\sum_{i=1}^{n_{\text{train}}}(y_i-g(x_i))^2}_{\text{Loss function}}+\lambda\underbrace{\int g''(t)^2\der t}_{\text{Regularizer}}$$
        \medskip
        The regularizer tries to control the second derivative of $g$ to be as small as possible to ensure smoothness.
        
        Assume that $\lambda$ is a extremely large value, the only solution is to let the integral to be 0, i.e. $g(t)$ should 
        be a linear function.
        \medskip

        If we put $\lambda=0$, then the solution will be $$\hat{g}(x_i)=y_i$$

        i.e. The model will have a perfect fit to the data.
        \medskip

        If we tune the parameter correctly, then we can get a good model that is neither overfit nor underfit.
    \end{example}
\end{expbox}

So how do we know if our tuning parameter is good or not?

Consider there is a set of data $\{(x_i,y_i)\}_{i=1...n}$, and we want to minimize $$\sum_{i=1}^{n}(y_i-g(x_i))^2+\lambda R(f)$$ as much as possible.

To achieve that, we need to spilt the data into two parts: Training data and Testing data, and the two set of datasets
should not be overlapping with each other.

The idea is, we only use the training data for solving optimization. After such process, we will 
obtain $\hat{g}$, which is estimated from the training data. The testing data is then used to evaluate whether the model is accurate or not. i.e. We would like to 
evaluate the value of $$\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{g}(x_i))^2$$
where $n$ is the number of testing data. Such value is defined as MSE (Mean Square Error).

\begin{defbox}
    \begin{definition}[Mean Square Error]
        $$MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{g}(x_i))^2$$ where $g(x_i)$ is 
        the prediction $\hat{g}$ gives for the $i-$th prediction.
    \end{definition}
\end{defbox}

\begin{center}
    \includegraphics*[scale=0.35]{Images/img2.png}
\end{center}

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    Black: Ground truth model & Red: Testing error                         \\ \hline
    Yellow: Simple linear model    & Gray: Training error \\ \hline
    Green: Perfect model (Fit all data into the model)    &  \\ \hline
    \end{tabular}
\end{table}

In the red curve, there are three point corresponding to the three different models on the left, which shows the MSE of overfitting
, underfitting and a good model. 

\begin{defbox}
    \begin{definition}[Training and Testing Error]
        Suppose $\hat{g}_\lambda$ is estimated from the data, and depends on $\lambda$, then the accuracy of $\hat{g}_\lambda$ based on the testing data is given by:
        $$\frac{1}{n_\text{Test}}\sum_{i\in\text{Test}}(y_i-\hat{g}_\lambda(x_i))^2$$
        This is called \textbf{testing mean square error (TMSE)}.

        \bigskip
        The \textbf{training error} is given by: $$\frac{1}{n_\text{Train}}\sum_{i\in\text{Train}}(y_i-\hat{g}_\lambda(x_i))^2$$
    \end{definition}
\end{defbox}

\begin{notebox}
    \begin{note}
        Training error is usually less than testing error, since optimization is solved with testing error, especially when you put $\lambda=0$.
    \end{note}
\end{notebox}

\begin{center}
    \includegraphics*[scale=0.35]{Images/img3.png}
\end{center}

Note that linear regression above provides a very poor fit to the data, while perfect fit model 
does not increase error too much, since the noise is very small. 

\begin{center}
    \includegraphics*[scale=0.35]{Images/img4.png}
\end{center}

For this dataset, the ground truth model is close to linear, and due to the large noise,
the high flexibility gives more error.

\newpage

\subsection{Bias-Variance Trade-Off}

As shown in the previous example, we can see that the testing error are in U-shape. To understand
the reason, we will need to introduce the concept of bias-variance trade-off.

Suppose we are interested in learning an unknown function $f(X)$ from the dataset $\mathcal{D}$,
namely $\hat{f}(X;\mathcal{D})$. Then at some future query point $X=x_0$, we want to
find $\hat{f}$, such that $\hat{f}$ satisfies: $$\min_{\hat{f}}[f(x_0)-\hat{f}(x_0;\mathcal{D})|X=x_0]^2$$

However are we really interested in evaluating this? Actually No! It is because the 
training dataset comes from a random selection! The prediction may not be accurate
if we changed another training dataset.

In order to solve the problem, we should take the random selection process in an \textbf{average sense}. 

Thus we are actually more interested in the following: $$\min_{\hat{f}}\evd[f(x_0)-\hat{f}(x_0;\mathcal{D})|X=x_0]^2$$

\begin{thmbox}
    \begin{theorem}
        \begin{eqnarray*}
            \evd[f(x_0)-\hat{f}(x_0;\mathcal{D})|X=x_0]^2&=&\underbrace{[f(x_0)-\evd(\hat{f}(x_0;\mathcal{D}))|X=x_0]^2}_{\text{Bias}^2}\\
            &+&\underbrace{\evd[[\evd(\hat{f}(x_0;\mathcal{D}))-\hat{f}(x_0;\mathcal{D})|X=x_0]^2]}_{\text{Variance}}
        \end{eqnarray*}
    \end{theorem}
    \begin{prfbox}
        \begin{proof}
            \begin{eqnarray*}
                [f(x_0)-\hat{f}(x_0,\mathcal{D})|X=x_0]^2 & = & [f(x_0)\underbrace{-\evd(\hat{f}(x_0;\mathcal{D}))+\evd(\hat{f}(x_0;\mathcal{D}))}_{=0}-\hat{f}(x_0,\mathcal{D})|X=x_0]^2\\
                &=& [f(x_0)-\evd(\hat{f}(x_0;\mathcal{D}))|X=x_0]^2 + [\evd(\hat{f}(x_0;\mathcal{D}))-\hat{f}(x_0,\mathcal{D})|X=x_0]^2\\
                &+& 2[f(x_0)-\evd(\hat{f}(x_0;\mathcal{D}))|X=x_0][\evd(\hat{f}(x_0;\mathcal{D}))-\hat{f}(x_0,\mathcal{D})|X=x_0]
            \end{eqnarray*}

            Now we take expectation to the expression above with respect to $\mathcal{D}$. We have:
            \begin{eqnarray*}
                &   &\evd[f(x_0)-\hat{f}(x_0,\mathcal{D})|X=x_0]^2\\
                & = & \underbrace{[f(x_0)-\evd(\hat{f}(x_0;\mathcal{D}))|X=x_0]^2}_{\text{Bias, as constant value}}\\
                & + & \underbrace{\evd\biggl([\evd(\hat{f}(x_0;\mathcal{D}))-\hat{f}(x_0,\mathcal{D})|X=x_0]^2\biggr)}_{\text{Variance}}\\
                & + & 2 \evd \biggl\{[f(x_0)-\evd(\hat{f}(x_0;\mathcal{D}))|X=x_0][\evd(\hat{f}(x_0;\mathcal{D}))-\hat{f}(x_0,\mathcal{D})|X=x_0]\biggr\}
            \end{eqnarray*}
            Consider the last term, we expand this term, then we will have: 
                \begin{eqnarray*}
                    && 2 \evd \biggl\{[f(x_0)-\evd(\hat{f}(x_0;\mathcal{D}))|X=x_0][\evd(\hat{f}(x_0;\mathcal{D}))-\hat{f}(x_0,\mathcal{D})|X=x_0]\biggr\} \\
                \end{eqnarray*}
        \end{proof}
    \end{prfbox}
\end{thmbox}

\begin{thmbox}
    \begin{prfbox}
        \begin{eqnarray*}
            &=& 2 \evd \biggl\{f(x_0)\evd[\hat{f}(x_0;\mathcal{D})]-f(x_0)\hat{f}(x_0;\mathcal{D})-[\evd[\hat{f}(x_0;\mathcal{D})]]^2+\hat{f}(x_0;\mathcal{D})\evd[\hat{f}(x_0;\mathcal{D})]\biggr\} \\
            &=& 2  \textcolor{red}{\biggl\{}\evd\biggl\{f(x_0)\evd[\hat{f}(x_0;\mathcal{D})]\biggr\}-\evd\biggl\{f(x_0)\hat{f}(x_0;\mathcal{D})\biggr\}-\evd\biggl\{[\evd[\hat{f}(x_0;\mathcal{D})]]^2\Biggr\} \\
            &+& \evd\biggl\{\hat{f}(x_0;\mathcal{D})\evd[\hat{f}(x_0;\mathcal{D})]\biggr\}\textcolor{red}{\Biggr\}}\\
            &=& \evd(f(x_0))\evd(\hat{f}(x_0;\mathcal{D}))-\evd(f(x_0))\evd(\hat{f}(x_0;\mathcal{D}))-\evd(\hat{f}(x_0;\mathcal{D}))^2+\evd(\hat{f}(x_0;\mathcal{D}))^2\\
            &=& 0
        \end{eqnarray*}
        The last term vanished, since we know that $\evd(f(x_0))=f(x_0)$ and $\evd(\evd(\hat{f}(x_0;\mathcal{D}))) = \evd(\hat{f}(x_0;\mathcal{D})).$

        \bigskip
        Also do note that even though $\hat{f}(X;\mathcal{D})$ is a random variable due to the randomness of the dataset, $\evd\hat{f}(X;\mathcal{D})$ is 
        no longer a random variable with respect to $\mathcal{D}$.

        \begin{expbox}
            \begin{example}
                If $Z\sim N(\mu,\sigma^2)$ is a random variable, then $\ev(Z)=\mu$ is no longer random variable.
            \end{example}
        \end{expbox}

    \end{prfbox}
\end{thmbox}
\begin{notebox}
    \begin{note}
        The reason for us to condition on $x_0$, is because we want to simplify the entire process.

        In fact, by using total expectation law, we are able to evaluate for $\evd\bigl[f(X)-\hat{f}(X;\mathcal{D})\bigr]^2$.

        \begin{thmbox}
            \begin{theorem}[Total Expectation Law]
                Given any random variables $X,Y$, then we have 
                $$\ev(X)=\ev_Y\Bigl(\ev(X|Y)\Bigr)$$ 
            \end{theorem}
        \end{thmbox}
        Thus we have: 
        \begin{eqnarray*}
            \evd\bigl[f(X)-\hat{f}(X;\mathcal{D})\bigr]^2 &=& \ev_X\biggl[\evd\bigl[f(X)-\hat{f}(X;\mathcal{D})\bigr]^2|X=x_0\biggr].
        \end{eqnarray*}
    \end{note}
\end{notebox}

\begin{notebox}
    \begin{note}
        Practically, $\evd\biggl(\hat{f}(x_0;\mathcal{D})\biggr)$ can be estimated by: 
        \begin{eqnarray*}
            \evd\biggl(\hat{f}(x_0;\mathcal{D})\biggr)&=&\frac{1}{B}\sum_{b=1}^B \hat{f}^{(b)}(x_0)
        \end{eqnarray*}
        From this, we can estimate the variance term by:
            \begin{eqnarray*}
                \evd\biggl[[\evd(\hat{f}(x_0;\mathcal{D}))-\hat{f}(x_0;\mathcal{D})|X=x_0]^2\biggr] &=& \frac{1}{B} \Biggl(\sum_{b=1}^B\biggl[\hat{f}(x_0)-\frac{1}{B}\sum_{b=1}^B\hat{f}^{(b)}(x_0)\biggr]\Biggr).
            \end{eqnarray*}

            This implies that, to estimate for variance, it is not necessary for us to know the ground-truth model. However, to know the bias, we do need to know the ground-truth model.
    \end{note}
\end{notebox}

\begin{defbox}
    \begin{definition}[Bias and Variance]
        Given an estimator $\hat{\theta}$ for some parameter $\theta$, the bias is defined as: 
        $$\text{Bias}(\hat{\theta},\theta)=\ev(\hat{\theta})-\theta$$
        while given a random variable $X$, the variance is defined as:
        $$
            \text{Var}(X)=\ev\bigl[(X-\ev(X))^2\bigr].
        $$
    \end{definition}
\end{defbox}

\begin{notebox}
    \begin{note}
        When we talk about variance here, such as $\var(\hat{\beta})$, mostly we means the variance \textbf{due to the change of the training data}.
    \end{note}
\end{notebox}

Suppose we are interested in a model that perfectly fits into the data. For example,

$$Y=f(X)+\varepsilon$$

This is the population-level model we seen before. Let $\text{Var}(\varepsilon)=\sigma^2$ and $\ev{(\varepsilon)}=0$,

We now take samples from such model, we will then have $(x_i,y_i)$ that is in relationship of:
\begin{eqnarray*}
    y_i=f(x_i)+\varepsilon_i &  & i=1,...,n
\end{eqnarray*}

We are interested in finding $g$, where $g$ can minimizes:

$$\sum_{i}(y_i-g(x_i))^2.$$

After solving the optimization problem, we will obtain $\hat{g}$, that $\hat{g}(x_i)=y_i, i=1,...,n$.

To adopt bias-variance view, we will have to evaluate $\ev(\hat{g})\text{ and Var}(\hat{g})$.

Note that:

\begin{eqnarray*}
    \ev(\hat{g}(x_i)) &=& \ev(y_i)\\
                      &=& \ev(f(x_i)+\varepsilon_i)\\
                      &=& \ev(f(x_i))\\
                      &=& f(x_i)
\end{eqnarray*}

To see if our estimator having bias or not, we only need to check $\ev(\hat{g}(x_i))-f(x_i)$.

However, since $\ev(\hat{g}(x_i))=f(x_i)$, thus $\ev(\hat{g}(x_i))-f(x_i)=0$. This implies if the model is very flexible
that can perfectly fits into the data, then the model is unbiased. 

Now, consider the variance, we have: 
\begin{eqnarray*}
    \var(\hat{g}(x_i)) &=& \var(y_i)\\
                       &=& \var(f(x_i)+\varepsilon_i)\\
                       &=& \var(\varepsilon_i)\\
                       &=& \sigma^2
\end{eqnarray*}
 
For a very flexible model, the bias can be very small, while the variance will be extremely large. 

\begin{center}
    \includegraphics*[scale=0.1]{Images/img5.jpg}
\end{center}

Suppose there is an dataset $\mathcal{D}$, and we want to solve the following optimization problem: $$\min_g\sum_{i=1}^{n}\bigl((y_i-g(x_i))\bigr)^2+\lambda\cdot R(g)$$

If $g(x_i)=C$ is a constant function (That is, totally not fitting into the data).

Since $\ev(\hat{g}(x_i))=C$, and $\var(\hat{g})=0$.

Thus 
\begin{eqnarray*}
    \bias &=&\ev(\hat{g}(x_0))-f(x_0)\\
          &=& C-f(x_0).
\end{eqnarray*}

This is not reducible, which implies the model is biased. However, the variance is 0. 

To conclude, we want to find a model, which can keeps a perfect balance between bias and variance. The
model may be good, even if the model is biased.

\begin{center}
    \includegraphics*[scale=0.3]{Images/img6.png}
\end{center}

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
           Red line: Test MSE     & Blue line: Bias                                       & Orange line: Variance                                        \\ \hline
    \end{tabular}
\end{table}

\section{Regression analysis}
\subsection{Simple Linear Regression}
\begin{defbox}
    \begin{definition}[Simple Linear Regression]
        In simple linear regression, there are only 1 variable, and thus there is only 2 parameters to fit in, which is:
        $$Y=\underbrace{\bt{0}+\bt{1}X}_{f(X)}+\varepsilon.$$
    \end{definition}
\end{defbox}

Suppose we want to estimate $\hat{\bt{0}},\hat{\bt{1}}$, then we have the realization value to be:

$$\hat{Y}=\bt{0}+\bt{1}X.$$

To figure out the estimate, suppose there is some observed training data $D=\biggl\{(x_i,y_i)\biggr\}_{i=1...n}$.

What we want to do now is to solve the optimization problem on the squared loss

$$(\hat{\bt{0}},\hat{\bt{1}})=\arg\min_{\bt{0},\bt{1}}\sum_{i=1}^n\bigl(y_i-\bt{0}-\bt{1}x_i\bigr)^2.$$

This problem is also known as least square problem, and the figure below shows the function that we want to find the optimized value.

\begin{center}
    \includegraphics*[scale=0.25]{Images/img7.png}
\end{center}

\begin{thmbox}
    \begin{theorem}[Least Square Problem]
        The solution to the least square problem is given by: 
    \end{theorem}
        \begin{eqnarray*}
            \hat{\bt{1}}&=& \ddfrac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\\
            \hat{\bt{0}}&=& \bar{y}- \hat{\bt{1}}\bar{x}
        \end{eqnarray*}
        \begin{prfbox}
            \begin{proof}
                I have a proof of this theorem, but there is not enough space in this margin to write it. \emoji{nerd-face}\emoji{point-up}
            \end{proof}
        \end{prfbox}
\end{thmbox}

\begin{prfbox}
    \begin{question*}
        Given $\mathcal{D}^{(1)}$, we can generate $\hat{\bt{0}}^{(1)}$ and $\hat{\bt{1}}^{(1)}$.

        Given $\mathcal{D}^{(2)}$, we can generate $\hat{\bt{0}}^{(2)}$ and $\hat{\bt{1}}^{(2)}$.
        etc.

        In an average sense, will $\hat{\bt{0}}^{(1)}=\bt{0}^*$? i.e. does $$\evd\bigl({\bt{0}}^{(1)}\bigr)=\bt{0}^*$$

        \begin{answer}
            Unbiased
        \end{answer}
    \end{question*}

\end{prfbox}

Consider the variance of $\hat{\bt{0}},\hat{\bt{1}}$, which is $\var(\hat{\bt{0}})$ and $\var(\hat{\bt{1}})$. We always wanted to keep the variance as small as possible. The following shows an example of the estimation of $\hat{\bt{0}}$ and $\hat{\bt{1}}$. Note that both of them are unbiased, however, the one on the right hand side obviously have a larger variance than the left hand side. 

\begin{center}
    \includegraphics*[scale=0.1]{Images/img8}
\end{center}

\begin{defbox}
    \begin{definition}[Quadratic Form]
        If $A$ is a positive definite matrix, then $f$ is called a quadratic from, if:
        $$f(x)=x^\intercal Ax$$
    \end{definition}
\end{defbox}

\begin{expbox}
    \begin{example}
        Consider the equation
        $$x_1^2+x_2^2=1.$$
        We can write this equation into quadratic form:
        $$\begin{bmatrix}
            x_1\\
            x_2
        \end{bmatrix}^\intercal
       \begin{bmatrix}
            1 & 0\\
            0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            x_1\\
            x_2
        \end{bmatrix}.
        $$
    \end{example}
\end{expbox}

With quadratic form, we can rewrite the least square problem as:

$$\sum_{i=1}^n (y_i-x_i^\intercal\beta)^2$$.

\begin{defbox}
    \begin{definition}[Residual Sum of Square (RSS)]
        The residual sum of square is the sum of the square of the error terms. i.e.
        \begin{eqnarray*}
            RSS&=&e_1^2+e_2^2+...+e_n^2\\
               &=&\sum_{i=1}^n(y_i-\hat{\bt{0}}-\hat{\bt{1}}x_i)^2
        \end{eqnarray*}
    \end{definition}
\end{defbox}

We would like to know how accurate is our estimated parameter as estimate of ground truth value. This can be done by computing the standard error these parameters.

\begin{thmbox}
    \begin{theorem}
        Define $SE$ as standard error. We have:
        \begin{eqnarray*}
            SE(\hat{\mu})^2&=&\frac{\sigma^2}{n}\\
            SE(\hat{\bt{0}})&=&\sigma^2\Biggl[\frac{1}{n}+\frac{\bar{x}^2}{\sum_i^n (x_i-\bar{x})^2}\Biggr]\\
            SE(\hat{\bt{1}})&=&\frac{\sigma^2}{\sum_i^n (x_i-\bar{x})^2}
        \end{eqnarray*}
        where $\sigma^2$ is estimated by $\hat{\sigma}^2=\frac{RSS}{n-2}$
    \end{theorem}
\end{thmbox} 

\begin{notebox}
    \begin{note}
        The reason why the denominator for $\hat{\sigma}^2$ is $n-2$, is because we are estimating 2 parameters. This is related to degree of freedom.
        Let's take a very simple example:
        \begin{expbox}
            \begin{example}
                Let $z_i\sim N(\mu,\sigma^2)$. Suppose we pick 3 data points: $\{z_1,z_2,z_3\}$. Then we can estimate the mean by the following formula:
                $$\hat{\mu}=\frac{1}{3}(z_1+z_2+z_3).$$
                Suppose we now know the value of $z_1,z_2$ and $\hat\mu$. Then we will also know the value of $z_3$. The number of sample is no longer 3, since
                when we know 2 of them, we immediately get the third one, with the constraint of the value of $\hat\mu$.
            \end{example}
        \end{expbox}
    \end{note}
\end{notebox}





\begin{defbox}
    \begin{definition}[$t$-statistics]
        Given the null hypothesis of 
        $$\mathcal{H}_0:\bt{1}=0,$$
        $t$-statistics is defined as:
        $$t=\frac{\hat{\bt{1}}-{\bt{1}}}{SE(\hat{\bt{1}})},$$
        which we have a $t$-distribution with $n-2$ degree of freedom.

    \end{definition}
\end{defbox}

We will reject the null hypothesis, if we observe a too large $|t|$, or $p$-value is 

To be continued... [2024-09-19, 30:00]

\end{document}

% Below are some boxes that may help with your document. They are not rendered during compilation.

% Definition
\begin{defbox}
    \begin{definition}[]

    \end{definition}
\end{defbox}

% Theorem (With a proof box included, remove if necessary)
\begin{thmbox}
    \begin{theorem}

    \end{theorem}
    \begin{prfbox}
        \begin{proof}
            
        \end{proof}
    \end{prfbox}
\end{thmbox} 

% Example
\begin{expbox}
    \begin{example}

    \end{example}
\end{expbox}

% Warning / Remarks
\begin{warnbox}
    \begin{warning}

    \end{warning}
\end{warnbox}

\begin{warnbox}
    \begin{remark}

    \end{remark}
\end{warnbox}

% Notes
\begin{notebox}
    \begin{note}

    \end{note}
\end{notebox}